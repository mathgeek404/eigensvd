#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 3cm
\rightmargin 3cm
\bottommargin 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section*
Motivation
\end_layout

\begin_layout Standard
Image recognition systems surpassing humans
\end_layout

\begin_layout Section*
The Problem 
\end_layout

\begin_layout Standard
Consider an N by M pixel image.
 We can unwrap this data matrix into a vector in an NM dimensional space
\end_layout

\begin_layout Standard
Each component in this NM space isn't very useful, just indicative of a
 single pixel value.
\end_layout

\begin_layout Standard
However, only a subset of points in this NM dimensional space correspond
 to structured images, potentially with some common uniform pattern
\end_layout

\begin_layout Standard
Classification of these images which we would like to predict (e.g.
 person, object)
\end_layout

\begin_layout Section*
PCA For EigenFaces
\end_layout

\begin_layout Standard
Goal: Find vectors which can best account for the variation of these images
 (e.g.
 faces)
\end_layout

\begin_layout Subsection*
Covariance Matrix
\end_layout

\begin_layout Standard
\begin_inset Formula $\mu=\frac{1}{n}\sum_{i=1}^{n}X_{i}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $cov(X_{i},X_{j})=E[(X_{i}-\mu_{i})(X_{i}-\mu_{j})]$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $X=[X_{i}-mu]_{i},i\in1,...,n$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $X=WDW^{T}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\hat{X}=USV^{T}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $XX^{T}=USV^{T}(USV^{T})^{T}=USV^{T}VS^{T}U^{T}=USIS^{T}U^{T}=USSU^{T}=US^{2}U^{T}$
\end_inset


\end_layout

\begin_layout Standard
So then 
\begin_inset Formula $U=W$
\end_inset

 and 
\begin_inset Formula $S^{2}=D$
\end_inset

.
 Thus the SVD manages to calculate the eigenvectors (and eigenvalues) without
 having to calculate the covariance matrix itself.
 
\end_layout

\begin_layout Standard
Hence, we can use the k largest singular values to indicate the k principle
 components from 
\begin_inset Formula $U$
\end_inset


\end_layout

\begin_layout Subsection*
Eigenfaces Methodology
\end_layout

\begin_layout Standard
Here, we assume X is the d by n data matrix of sample points, where d is
 the number of features/variables and n is the number of samples.
\end_layout

\begin_layout Standard
1.
 Compute the mean of the data points: 
\begin_inset Formula $\mu=\frac{1}{n}\sum_{i=1}^{n}x_{i}$
\end_inset


\end_layout

\begin_layout Standard
2.
 Center the matrix X by subtracting mu from each column: 
\begin_inset Formula $X=[X_{i}-mu]_{i},i\in1,...,n$
\end_inset


\end_layout

\begin_layout Standard
3.
 Take the SVD of X
\end_layout

\begin_layout Standard
4.
 
\end_layout

\begin_layout Subsection*
Projector and Recognition
\end_layout

\begin_layout Subsection*
Limitations? Issues
\end_layout

\begin_layout Subsection*
Fisherfaces
\end_layout

\begin_layout Standard
An attempt to integrate classification data into the process
\end_layout

\begin_layout Subsection*
LDA methodology
\end_layout

\begin_layout Standard
Suppose we are given a set of samples 
\begin_inset Formula $S=\{(x_{1},c_{1}),...,(x_{n},c_{n})\}$
\end_inset

, with each 
\begin_inset Formula $x_{i}$
\end_inset

 with some classification 
\begin_inset Formula $c_{i}\in\{1,...,c\}$
\end_inset

.
 Define 
\begin_inset Formula $ind(S,r):=\{i|i\in\{1,..,n\},(x_{i},r)\in S\}$
\end_inset

 (that is, the set of indices for samples with the given classification).
\end_layout

\begin_layout Standard
Calculate 
\begin_inset Formula $S_{B}=\sum_{i=1}^{c}N_{i}(\mu_{i}-\mu)(\mu_{i}-\mu)^{T}$
\end_inset


\end_layout

\begin_layout Standard
Calculate 
\begin_inset Formula $S_{B}=\sum_{i=1}^{c}\sum_{i\in ind(S,i)}N_{i}(\mu_{i}-\mu)(\mu_{i}-\mu)^{T}$
\end_inset


\end_layout

\begin_layout Standard
Want to find the projection matrix that maximizes:
\end_layout

\begin_layout Standard
\begin_inset Formula $W_{opt}=argmax_{W}\frac{|W^{T}S_{B}W|}{|W^{T}S_{W}W|}$
\end_inset


\end_layout

\begin_layout Standard
Project sample image vector 
\begin_inset Formula $x$
\end_inset

 onto this subspace via 
\begin_inset Formula $\hat{y}=W^{T}(x-\mu)$
\end_inset

.
 
\end_layout

\end_body
\end_document
